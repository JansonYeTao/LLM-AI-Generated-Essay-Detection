# LLM Generated Text Detection

# Train & Test Data Description:


The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM.

All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.

Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples. You may wish to generate more essays to use as training data.

Please note that this is a Code Competition. The data in test_essays.csv is only dummy data to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 9,000 essays in the test set, both student written and LLM generated.




Folks report that train and test data come from persuade corpus2 and this was shared at discussion threads:
- https://www.kaggle.com/datasets/nbroad/persaude-corpus-2
- https://www.kaggle.com/code/nbroad/persuade-train-essays-analysis
- https://www.kaggle.com/code/nbroad/persuade-train-essays-analysis/notebook
  

<br>



# Difficulty in Modeling

We can leverage AI-generated data to train a classifier. However, we will have following issue.

1. Distribution shift between data generated by our own and data generated by competition provider.
   1. could be different in prompt, LLM models, generation hyperparameter eg, temperature, and post-processing
2. Distribution shift of public test data set and private dataset
   1. All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.
   2. Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples. You may wish to generate more essays to use as training data.
   3. 



# Proposed Solution:

## Classifier Pretraining 

The purpose of this step is to move the general foundation language model a bit closer to the kaggle competition domain context (detect AI & Human writing). This because language model like deberta are trained general purpose (using surrounding context to predict center word),  which will alleviate the distribution difference.


train_neg_list.pickle and train_pos_list.pickle are around 500,000 pairs for pretraining classifiers. Positive are human created and Negative examples are AI-Generated.

We then train deberta on this dataset weakly (eg. one epoch) to move this language model a bit closer to the kaggle competition domain context (detect AI & Human writing)


## Data Generation for Pretraining: 

see folder: gaigtdatagenerationforpretrain

train_neg_list.pickle and train_pos_list.pickle are around 500,000 pairs for pretraining classifiers. Positive are human created (comes from SlimPajama dataset) and Negative examples are AI-Generated (based on the SlimPajama data).

Steps on how train_neg_list and train_pos_list got gathered:

1. Acquire the SlimPajama C4 subset.
2. Filter to keep only documents with word length > 2048 so that you have plenty of text for the prompt and next-token generation. 
3. Random chunk selection and prompt creation.
4. Randomly choose a truncation point to define a 1024-token “prompt” from the text.
5. The next 1024 tokens (the “continuation”) remain as the “human” text in your dataset.
Generate AI responses using various LLMs.
6. Feed each 1024-token prompt into an LLM and request the next 1024 tokens, forcing it to generate text of equal length. Store the generated text as “AI” data.
7. Assemble final pairs into positive (AI-generated) vs. negative (human-extracted) text samples (e.g. train_pos_list, train_neg_list).


In case you are interested in how to generate train_neg_list.pickle and train_pos_list.pickle, everything is in the gaigtdatagenerationforpretrain/ folder. Perform the following steps:
1) Download the SlimPajama dataset.
2) Run preprocess_external_chunk1-10.py for file selection and random chunking. Only C4 subset was used, only files with word length > 2048 was used, because I want to make sure the LLMs have 1024 tokens as prompt and generate the next 1024 tokens.
3) Run the python files in every folders with LLM names. Note that some files may error because I forgot to add padding. I was able to only run roughly 90% of those files.



## Classifier Pretraining


Model Selection: Use DeBERTa-v3-large or DeBERTa-large as the base model.

Training Setup:
1. Load the ~500,000 positive/negative text pairs.
2. Fine-tune for one epoch, with maximum length set to around 768 tokens (or whichever your GPU can handle).
3. Save your pretrained classifier.

17/, 19/, 20/ in the code/ folder are for classifier pretraining, you need to run them first.



> Notes: This "Pretraining" approach including data generation process for pretraining is a general way of what we can do for any setting. eg. in chatbot cases, we can adapt this method in the same way to get more synthetic data. (Notice that in chat setting, input text data will be often much shorter than setting of writing essay)




# Classifier Fine-tuning

Move the model even closer to that specific 5 prompts that will be used in test set.

> Notes: This step is particularly design for this kaggle competition which the final goal will be close the gap between the participant's generated data and competition organizer's generated data, eg. we're supposed not to know what test data and which prompt the organizer will be eventually using for evaluating.
> 
> However, this is quite common for kaggle competition, which people will leverage "data leakage" information to boost their score on the public & private leaderboard by reverse engineering out the extra info about test dataset.
>
> In real work business working setting, we often don't have that data leakage information, and instead many times we try to avoid that kind of leakage.


## Data Generation for Finetuning (Prompt-Specific)

Move the model even closer to that specific 5 prompts that will be used in test set.


I did language model (LM) fine tuning on the student essays so the LLMs could generate texts mimicking students' writing, including citations from the original articles and even typos.

> 

> Notes, generate texts mimicking students' writing, including citations from the original articles and even typos.
>
> We can think of this as a way of generating more hard positive cases so that classifier can be more robust.
> 
> This could also be used for to simulate real use-case eg. we can fine-tune LLM on real customer chat production data to generate simulated question, and then QA testing out chatbot performance etc.


How we know what prompts will be used in test set?

> See answer here: https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/453410
>
> and Folks are seeking data leakage elsewhere which normally we shouldnt do such thing but its totally fine for kaggle competition.


1. Gather the five competition prompts (the topics actually used in the test set). 
2. Collect real student essays for those prompts (the negative class).
3. Finetune an LLM on those real essays (so the LLM itself learns typical student style, references to articles, typos, etc.). Tools like h2o-llmstudio can be used—no heavy code is required if you follow their GUI approach.
4. Generate new AI texts by prompting the finetuned LLM with the same five prompts, sampling at various temperatures.
5. Assemble the final training set for the second-stage classifier finetuning (consisting of real essays vs. newly generated essays, focusing on those five prompts).
6. train_df.csv is for last step finetuning.



To generate the train_df.csv, go to daigtdatagenerationforfinetune/ and perform the following steps:
1) Install h2o-llmstudio
2) Run prepare_data_5_promts.py to generate input file for finetuning. Only essays of the 5 prompts in test set were included.
3) Perform finetuning. The config files are in the folders inside llmstudio_configs/. Those folders are named after the LLMs used.
4) Run all the python files with LLM names
5) Run prepare_train_data.py for assembling.



Important thing is that student's essays are written in response to some context questions, which means LLM need to also know that information otherwise it will be easy to distinguish the difference. (More hard negative.)

Basically the generated essay should be not only rooted in the given prompt, but also be rooted in given context questions. This is to make sure when we compare only on the text which is written by Human or AI instead of other outside factors.



## Classifier Finetuning (Prompt-Specific)
Start from the pretrained DeBERTa weights (the general “human vs. AI” model).

Load the new data (student vs. newly generated for the 5 prompts).
Run another epoch or so of training to adapt the classifier specifically to the style of these prompts.
Also consider external datasets (like “v4data” or other shared Kaggle sets) to broaden coverage if available.


_ft1/ are for finetuning on train_v4_drcat_01.csv and train_v4_drcat_01.csv can be downloaded from https://www.kaggle.com/datasets/thedrcat/daigt-v4-train-dataset


_ft103/ are for finetuning on train_df.csv, and train_df.csv are generated in  Data Generation for Finetuning (Prompt-Specific) Steps`

